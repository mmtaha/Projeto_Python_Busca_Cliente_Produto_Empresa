{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mmtaha/Projeto_Python_Busca_Cliente_Produto_Empresa/blob/main/Projeto_Oracle_Busca_Empresa_Produto_Oppty_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgHtGjMZRT5J"
      },
      "outputs": [],
      "source": [
        "# BLOCO 1: IMPORTAÇÕES E CONFIGURAÇÕES INICIAIS\n",
        "\n",
        "# Se estiver em Jupyter e precisar instalar dependências, descomente:\n",
        "!pip install pyxlsb unidecode rapidfuzz scikit-learn xlsxwriter --quiet\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import unidecode\n",
        "from functools import lru_cache\n",
        "from IPython.display import display\n",
        "from collections import Counter\n",
        "\n",
        "# ==== Configurações iniciais ====\n",
        "try:\n",
        "    from rapidfuzz import fuzz, process, distance\n",
        "    USING = \"rapidfuzz\"\n",
        "    print(\"ℹUsando rapidfuzz\")\n",
        "except Exception:\n",
        "    from fuzzywuzzy import fuzz, process\n",
        "    USING = \"fuzzywuzzy\"\n",
        "    distance = None\n",
        "    print(\"ℹUsando fuzzywuzzy\")\n",
        "\n",
        "try:\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    HAVE_SKLEARN = True\n",
        "    print(\"ℹCosine check habilitado\")\n",
        "except Exception:\n",
        "    HAVE_SKLEARN = False\n",
        "    print(\"ℹscikit-learn não disponível\")\n",
        "\n",
        "# ==== Parâmetros ====\n",
        "MATCH_THRESHOLD = 95\n",
        "JW_MIN = 90\n",
        "COSINE_MIN = 0.95\n",
        "TOPN_PER_ROW = 40\n",
        "\n",
        "# ==== PALAVRAS-CHAVE PRIORITÁRIAS ESPECÍFICAS PEPSICO ====\n",
        "PALAVRAS_CHAVE_PRIORITARIAS = [\n",
        "    # Nomes exatos da PepsiCo\n",
        "    \"pepsico\", \"pepsi-cola\", \"pepsicola\", \"pepsi cola\",\n",
        "    # Marcas principais da PepsiCo\n",
        "    \"sabritas\", \"gamesa\", \"quaker\", \"gatorade\", \"ruffles\",\n",
        "    \"cheetos\", \"doritos\", \"lays\", \"frito-lay\", \"mirinda\",\n",
        "    \"7up\", \"sonrics\", \"emmanuel\", \"barcel\", \"tostitos\",\n",
        "    \"tropicana\", \"manaos\", \"alvalle\", \"happy\",\n",
        "    \"mabel\", \"elma chips\", \"evercrisp\", \"fruko\", \"holy\",\n",
        "    \"kero\", \"matutino\", \"mayco\", \"natural hit\",\n",
        "    \"natu\", \"paps\", \"paseo\", \"pimpollo\", \"rancrisp\",\n",
        "    \"ricky\", \"ritmo\", \"rosquinhas\", \"salmas\", \"salticas\",\n",
        "    \"santa maria\", \"santiveri\", \"smacks\", \"tosty\", \"tranquitas\",\n",
        "    \"trident\", \"trocipollo\", \"twist\", \"vive 100\", \"yogu yogu\",\n",
        "    # Subsidiárias operacionais\n",
        "    \"alimentos del istmo\", \"alimesa\", \"comercializadora nacional\",\n",
        "    \"corporativo internacional mexicano\", \"latin american holdings\",\n",
        "    \"snacks america latina\", \"bebidas sudamerica\", \"inversiones borneo\",\n",
        "    \"inversiones pfi\", \"servicios gbf\", \"servicios gflg\", \"servicios syc\",\n",
        "    \"pepsico alimentos\", \"pepsico alimentos mexico\", \"pepsico argentina\",\n",
        "    \"pepsico brasil\", \"pepsico chile\", \"pepsico colombia\", \"pepsico mexico\",\n",
        "    \"pepsico venezuela\", \"pepsico andina\", \"pepsico latina\", \"pepsico caribe\",\n",
        "    \"productos pepsi\", \"bebidas pepsi\", \"refrescos pepsi\", \"jugos pepsi\",\n",
        "    \"snacks pepsi\", \"alimentos pepsi\"\n",
        "]\n",
        "\n",
        "# ==== PALAVRAS PARA EXCLUSÃO CONTEXTUAL RIGOROSA ====\n",
        "PALAVRAS_EXCLUSAO_CONTEXTUAL = [\n",
        "    \"barcelos\", \"barcelona\", \"barclays\", \"tropicana hotel\", \"tropicana resort\",\n",
        "    \"tropicana casino\", \"starbucks store\", \"starbucks cafe\", \"starbucks coffee\",\n",
        "    \"hotel\", \"resort\", \"casino\", \"restaurant\", \"cafe\", \"coffee\", \"university\",\n",
        "    \"school\", \"hospital\", \"clinic\", \"government\", \"municipal\", \"federal\",\n",
        "    \"estadual\", \"health\", \"medical\", \"insurance\", \"bank\", \"banco\", \"finance\",\n",
        "    \"investment\", \"real estate\", \"construction\", \"engineering\", \"consulting\",\n",
        "    \"law\", \"legal\", \"accounting\", \"education\", \"research\", \"technology\",\n",
        "    \"software\", \"hardware\", \"it\", \"telecom\", \"media\", \"entertainment\", \"sports\",\n",
        "    \"travel\", \"tourism\", \"transport\", \"logistics\", \"energy\", \"power\", \"water\",\n",
        "    \"utility\", \"pharma\", \"pharmaceutical\", \"medical\", \"healthcare\", \"insurance\"\n",
        "]\n",
        "\n",
        "# ==== FUNÇÕES DE NORMALIZAÇÃO ====\n",
        "def base_normalize(text: str) -> str:\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    s = unidecode.unidecode(str(text).lower())\n",
        "    s = re.sub(r\"[\\(\\[{].*?[\\)\\]}]\", \" \", s)\n",
        "    s = re.sub(r\"[^\\w\\s]\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# CORREÇÃO AQUI - padrão regex simplificado\n",
        "SEP = r\"[\\s.,/\\-_(){};:]+\"\n",
        "\n",
        "def flex_pat(phrase: str) -> str:\n",
        "    tokens = phrase.strip().split()\n",
        "    return r\"\\b\" + (SEP.join([re.escape(t) for t in tokens])) + r\"\\b\"\n",
        "\n",
        "# ==== SUFIXOS CORPORATIVOS ====\n",
        "CORPORATE_SUFFIXES = [\n",
        "    \"ltda\", \"limitada\", \"me\", \"eireli\", \"s/a\", \"sa\", \"s.a\", \"s a\", \"cia\", \"companhia\",\n",
        "    \"empresa\", \"comercio\", \"comércio\", \"servicos\", \"serviços\", \"industria\", \"industrial\",\n",
        "    \"distribuidora\", \"sas\", \"s.a.s\", \"s a s\", \"srl\", \"s.r.l\", \"s r l\", \"sa de cv\",\n",
        "    \"s.a. de c.v\", \"s a de c v\", \"eurl\", \"e.u.r.l\", \"sc\", \"s.c\", \"sp\", \"s.p\", \"scs\",\n",
        "    \"s.c.s\", \"senc\", \"s.en.c\", \"sarl\", \"s.a.r.l\", \"inc\", \"incorporated\", \"corp\",\n",
        "    \"corporation\", \"ltd\", \"limited\", \"llc\", \"lp\", \"l.p\", \"pllc\", \"llp\", \"plc\", \"pty\",\n",
        "    \"pte\", \"s a\", \"gp\", \"companies\", \"pc\", \"p.c\", \"lc\", \"l.c\", \"lllp\", \"gmbh\", \"ag\",\n",
        "    \"kg\", \"kgaa\", \"ggmbh\", \"sarl\", \"sas\", \"eurl\", \"snc\", \"s.n.c\", \"scs\", \"sca\", \"s.c.a\",\n",
        "    \"srl\", \"spa\", \"s.p.a\", \"snc\", \"sas\", \"bv\", \"nv\", \"vof\", \"cv\", \"ab\", \"oy\", \"as\",\n",
        "    \"aps\", \"kg\", \"zrt\", \"nyrt\", \"co ltd\", \"company limited\", \"pt\", \"pvt ltd\",\n",
        "    \"private limited\", \"kk\", \"yk\", \"gk\", \"sdn bhd\", \"bhd\", \"cc\", \"pty ltd\", \"shma\",\n",
        "    \"llc\", \"pjsc\", \"asc\", \"nl\", \"group\", \"international\", \"worldwide\", \"global\",\n",
        "    \"ventures\", \"enterprises\"\n",
        "]\n",
        "\n",
        "NON_DISTINCTIVE_WORDS = [\n",
        "    \"group\", \"grupo\", \"grup\", \"gr\", \"gr.\", \"international\", \"internacional\", \"intl\",\n",
        "    \"global\", \"worldwide\", \"world\", \"companies\", \"enterprises\", \"ventures\", \"partners\",\n",
        "    \"partnership\", \"management\", \"capital\", \"investment\", \"investments\", \"fund\", \"trust\",\n",
        "    \"services\", \"solutions\", \"technologies\", \"technology\", \"tec\", \"tech\", \"tecno\",\n",
        "    \"systems\", \"system\", \"industries\", \"industry\", \"industrial\", \"comercial\", \"commercial\",\n",
        "    \"financeiro\", \"financial\", \"retail\", \"varejo\", \"atacado\", \"wholesale\", \"manufacturing\",\n",
        "    \"manufactura\", \"inversiones\", \"Ltd.\", \"Limited\", \"S.R.L.\", \"SRL\", \"LLC\", \"Inc.\",\n",
        "    \"N.V.\", \"Logistica\", \"Holding\", \"Holdings\", \"LOGISTICS\", \"Servicios\", \"BANCO\",\n",
        "    \"S/A\", \"Group\", \"Comercializadora\", \"Seguros\", \"health\", \"healthcare\", \"pharma\",\n",
        "    \"pharmaceutical\", \"medical\", \"med\", \"energy\", \"energia\", \"power\", \"utilities\",\n",
        "    \"utility\", \"sports\", \"deportivo\", \"entertainment\", \"media\", \"communications\",\n",
        "    \"telecom\", \"construction\", \"construção\", \"engineering\", \"engenharia\", \"imobiliario\",\n",
        "    \"property\", \"properties\", \"hotel\", \"hospitality\", \"travel\", \"viagens\", \"tour\",\n",
        "    \"tourism\", \"auto\", \"automotive\", \"car\", \"vehicle\", \"motor\", \"insurance\", \"seguros\",\n",
        "    \"bank\", \"banco\", \"financial\", \"finanças\", \"asset\", \"assets\", \"security\", \"segurança\",\n",
        "    \"software\", \"hardware\", \"it\", \"information technology\", \"data\", \"digital\", \"online\",\n",
        "    \"web\", \"net\", \"network\", \"cloud\", \"consulting\", \"consultoria\", \"advisory\", \"law\",\n",
        "    \"legal\", \"accounting\", \"contabilidade\", \"audit\", \"auditoria\", \"education\", \"educação\",\n",
        "    \"school\", \"university\", \"research\", \"pesquisa\", \"development\", \"desenvolvimento\",\n",
        "    \"design\", \"planning\", \"planejamento\", \"marketing\", \"advertising\", \"publicidade\",\n",
        "    \"sales\", \"vendas\", \"support\", \"suporte\", \"care\", \"cuidado\", \"home\", \"casa\", \"office\",\n",
        "    \"escritorio\", \"business\", \"negocio\", \"corp\", \"company\", \"co\", \"com\", \"and\", \"e\", \"y\",\n",
        "    \"of\", \"de\", \"do\", \"da\", \"das\", \"dos\", \"the\", \"el\", \"la\", \"los\", \"las\", \"le\", \"les\",\n",
        "    \"en\", \"et\", \"und\", \"&\"\n",
        "]\n",
        "\n",
        "_base_suffix_phrases = CORPORATE_SUFFIXES + NON_DISTINCTIVE_WORDS\n",
        "_suffix_regexes = [re.compile(flex_pat(p), flags=re.IGNORECASE) for p in _base_suffix_phrases]\n",
        "\n",
        "_tail_suffix_tokens = (\n",
        "    {p.strip().lower() for p in _base_suffix_phrases if p and p.strip()} |\n",
        "    {\"cv\", \"rl\", \"c\", \"v\", \"de\", \"por\", \"a\", \"s\", \".a\", \"b\", \"d\", \".d\", \"m\", \"l\", \"k\",\n",
        "     \"j\", \"i\", \"h\", \"e\", \"f\", \"n\", \"g\", \"o\", \"p\", \"q\", \"r\", \"t\", \"u\", \"x\", \"y\", \"z\", \"w\"}\n",
        ")\n",
        "_SUFFIX_SET_NORM = {p.strip().lower() for p in _base_suffix_phrases if p and p.strip()}\n",
        "\n",
        "# ==== FUNÇÕES DE LIMPEZA ====\n",
        "def strip_suffixes(s: str) -> str:\n",
        "    if pd.isna(s):\n",
        "        return \"\"\n",
        "    out = str(s)\n",
        "    for rgx in _suffix_regexes:\n",
        "        out = rgx.sub(\" \", out)\n",
        "    out = re.sub(r\"\\s+\", \" \", out).strip()\n",
        "    words = out.split()\n",
        "    cut = 0\n",
        "    while words and cut < 5 and words[-1].lower() in _tail_suffix_tokens:\n",
        "        words.pop()\n",
        "        cut += 1\n",
        "    return \" \".join(words).strip()\n",
        "\n",
        "def remove_suffix_tokens(text: str) -> str:\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    words = text.split()\n",
        "    filtered = []\n",
        "    for w in words:\n",
        "        wl = w.lower().strip(\".,;\")\n",
        "        if wl in _tail_suffix_tokens or wl in _SUFFIX_SET_NORM:\n",
        "            continue\n",
        "        filtered.append(w)\n",
        "    return \" \".join(filtered)\n",
        "\n",
        "@lru_cache(maxsize=8)\n",
        "def _compile_country_regexes(country_tuple):\n",
        "    norms = []\n",
        "    for c in country_tuple:\n",
        "        cn = base_normalize(c)\n",
        "        if cn:\n",
        "            norms.append(cn)\n",
        "    norms = sorted(set(norms), key=len, reverse=True)\n",
        "    return [re.compile(flex_pat(cn), flags=re.IGNORECASE) for cn in norms]\n",
        "\n",
        "def remove_countries(text: str, country_list=None) -> str:\n",
        "    if not text or not country_list:\n",
        "        return text\n",
        "    regs = _compile_country_regexes(tuple(country_list))\n",
        "    out = text\n",
        "    for rgx in regs:\n",
        "        out = rgx.sub(\" \", out)\n",
        "    return re.sub(r\"\\s+\", \" \", out).strip()\n",
        "\n",
        "def clean_name(name, country_list=None):\n",
        "    if pd.isna(name):\n",
        "        return \"\"\n",
        "    s = base_normalize(name)\n",
        "    s = strip_suffixes(s)\n",
        "    s = remove_suffix_tokens(s)\n",
        "    if country_list:\n",
        "        s = remove_countries(s, country_list)\n",
        "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "def remove_parentheses(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    return re.sub(r\"[\\(\\[{].*?[\\)\\]}]\", \"\", str(text)).strip()\n",
        "\n",
        "# ==== FUNÇÕES DE TOKENIZAÇÃO ====\n",
        "EXTRA_STOP = set(CORPORATE_SUFFIXES + NON_DISTINCTIVE_WORDS)\n",
        "\n",
        "def tokenize(s: str):\n",
        "    if not s:\n",
        "        return set()\n",
        "    toks = re.split(r\"\\s+\", s)\n",
        "    return {t for t in toks if len(t) >= 3 and t not in EXTRA_STOP}\n",
        "\n",
        "def generate_candidate_idx(target_clean: str, subs_clean_list, subs_tokens_list):\n",
        "    targets = tokenize(target_clean)\n",
        "    cand_idx = []\n",
        "    if not targets:\n",
        "        return cand_idx\n",
        "    tgt_str = target_clean\n",
        "    for i, cand in enumerate(subs_clean_list):\n",
        "        if not cand:\n",
        "            continue\n",
        "        ctoks = subs_tokens_list[i]\n",
        "        if targets.intersection(ctoks):\n",
        "            cand_idx.append(i)\n",
        "        elif len(tgt_str) >= 5 and (tgt_str in cand or cand in tgt_str):\n",
        "            cand_idx.append(i)\n",
        "    return cand_idx\n",
        "\n",
        "# ==== FUNÇÕES DE SIMILARIDADE ====\n",
        "def robust_scores(a: str, b: str):\n",
        "    if USING == \"rapidfuzz\":\n",
        "        ts_set = fuzz.token_set_ratio(a, b)\n",
        "        ts_sort = fuzz.token_sort_ratio(a, b)\n",
        "        part = fuzz.partial_ratio(a, b)\n",
        "        if distance is not None:\n",
        "            try:\n",
        "                jw = distance.JaroWinkler.normalized_similarity(a, b)\n",
        "            except Exception:\n",
        "                jw = ts_set / 100.0\n",
        "            jw = jw * 100 if jw <= 1.0 else jw\n",
        "        else:\n",
        "            jw = ts_set\n",
        "    else:\n",
        "        ts_set = fuzz.token_set_ratio(a, b)\n",
        "        ts_sort = fuzz.token_sort_ratio(a, b)\n",
        "        part = fuzz.partial_ratio(a, b)\n",
        "        jw = ts_set\n",
        "    best = max(ts_set, ts_sort, part)\n",
        "    return best, jw, ts_set, ts_sort, part\n",
        "\n",
        "def cosine_ok(a: str, b: str):\n",
        "    if not HAVE_SKLEARN:\n",
        "        return None\n",
        "    vec = TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3,5))\n",
        "    X = vec.fit_transform([a, b])\n",
        "    cos = float(cosine_similarity(X[0], X[1])[0,0])\n",
        "    return cos\n",
        "\n",
        "def accept_pair(a: str, b: str):\n",
        "    best, jw, ts_set, ts_sort, part = robust_scores(a, b)\n",
        "    if best < MATCH_THRESHOLD:\n",
        "        return False, best, jw, None\n",
        "    if jw < JW_MIN:\n",
        "        cos = cosine_ok(a, b)\n",
        "        if cos is None or cos < COSINE_MIN:\n",
        "            return False, best, jw, cos\n",
        "        return True, best, jw, cos\n",
        "    else:\n",
        "        cos = cosine_ok(a, b)\n",
        "        if cos is not None and cos < 0.60 and best < (MATCH_THRESHOLD + 5):\n",
        "            return False, best, jw, cos\n",
        "        return True, best, jw, cos\n",
        "\n",
        "# ==== FUNÇÃO DE BUSCA CONTEXTUAL INTELIGENTE ====\n",
        "def busca_contextual_inteligente(texto, priority_words):\n",
        "    \"\"\"\n",
        "    Função ESPECÍFICA para identificar apenas subsidiárias PepsiCo reais\n",
        "    com verificação rigorosa para evitar falsos positivos\n",
        "    \"\"\"\n",
        "    if pd.isna(texto):\n",
        "        return False, None\n",
        "\n",
        "    texto_normalizado = unidecode.unidecode(str(texto).lower())\n",
        "\n",
        "    # 1. Exclusão rigorosa de entidades não-PepsiCo\n",
        "    for palavra in PALAVRAS_EXCLUSAO_CONTEXTUAL:\n",
        "        if re.search(rf'\\b{re.escape(palavra)}\\b', texto_normalizado):\n",
        "            return False, None\n",
        "\n",
        "    # 2. Verificar apenas combinações MUITO ESPECÍFICAS da PepsiCo\n",
        "    combinacoes_pepsico_especificas = [\n",
        "        (\"pepsi\", \"comercializadora\"), (\"pepsico\", \"comercializadora\"),\n",
        "        (\"pepsi\", \"alimentos\"), (\"pepsico\", \"alimentos\"),\n",
        "        (\"pepsi\", \"bebidas\"), (\"pepsico\", \"bebidas\"),\n",
        "        (\"pepsi\", \"snacks\"), (\"pepsico\", \"snacks\"),\n",
        "        (\"pepsi\", \"distribuidora\"), (\"pepsico\", \"distribuidora\"),\n",
        "        (\"refrescos\", \"pepsi\"), (\"bebidas\", \"pepsi\"),\n",
        "        (\"productos\", \"pepsi\"), (\"alimentos\", \"pepsi\"),\n",
        "        (\"comercializadora\", \"nacional\", \"pepsi\"),\n",
        "        (\"comercializadora\", \"nacional\", \"pepsico\"),\n",
        "        (\"corporativo\", \"internacional\", \"mexicano\", \"pepsico\"),\n",
        "        (\"latin\", \"american\", \"holdings\", \"pepsico\"),\n",
        "        (\"snacks\", \"america\", \"latina\", \"pepsico\"),\n",
        "        (\"bebidas\", \"sudamerica\", \"pepsico\")\n",
        "    ]\n",
        "\n",
        "    for combinacao in combinacoes_pepsico_especificas:\n",
        "        if all(re.search(rf'\\b{re.escape(p)}\\b', texto_normalizado) for p in combinacao):\n",
        "            return True, f\"Combinação: {combinacao}\"\n",
        "\n",
        "    # 3. Verificar apenas marcas MUITO ESPECÍFICAS da PepsiCo (exatas)\n",
        "    marcas_pepsico_exatas = [\n",
        "        \"pepsico\", \"pepsi-cola\", \"pepsicola\", \"sabritas\", \"gamesa\",\n",
        "        \"quaker\", \"gatorade\", \"ruffles\", \"cheetos\", \"doritos\",\n",
        "        \"lays\", \"frito-lay\", \"mirinda\", \"7up\", \"sonrics\", \"emmanuel\",\n",
        "        \"barcel\", \"tostitos\", \"tropicana\", \"manaos\", \"alvalle\", \"lucas\"\n",
        "    ]\n",
        "\n",
        "    # Busca EXATA das marcas (não substring)\n",
        "    for marca in marcas_pepsico_exatas:\n",
        "        if re.search(rf'\\b{re.escape(marca)}\\b', texto_normalizado):\n",
        "            return True, f\"Marca: {marca}\"\n",
        "\n",
        "    # 4. Verificar múltiplas palavras-chave no contexto (pelo menos 2)\n",
        "    palavras_encontradas = []\n",
        "    for keyword in priority_words:\n",
        "        keyword_normalized = unidecode.unidecode(keyword.lower())\n",
        "        # Buscar palavra exata, não substring\n",
        "        if re.search(rf'\\b{re.escape(keyword_normalized)}\\b', texto_normalizado):\n",
        "            palavras_encontradas.append(keyword_normalized)\n",
        "            if len(palavras_encontradas) >= 2:\n",
        "                return True, f\"Palavras: {', '.join(palavras_encontradas)}\"\n",
        "\n",
        "    return False, None\n",
        "\n",
        "# ==== BUSCA PRIORITÁRIA COMPLETA EM CAMADAS ====\n",
        "def busca_prioritaria_completa_camadas(list_mapping, priority_words):\n",
        "    \"\"\"\n",
        "    Busca TODAS as palavras-chave prioritárias em camadas hierárquicas\n",
        "    Retorna TODOS os matches encontrados em cada camada\n",
        "    \"\"\"\n",
        "    priority_results = []\n",
        "    linhas_processadas = set()\n",
        "\n",
        "    print(\"🔍 ETAPA 0.1 - Busca prioritária COMPLETA em Global Ultimate Name...\")\n",
        "    if \"Global Ultimate Name\" in list_mapping.columns:\n",
        "        for idx, name in enumerate(list_mapping[\"Global Ultimate Name\"]):\n",
        "            if idx not in linhas_processadas:\n",
        "                resultado, palavra_match = busca_contextual_inteligente(name, priority_words)\n",
        "                if resultado:\n",
        "                    priority_results.append({\n",
        "                        'lm_idx': idx,\n",
        "                        'coluna': 'Global Ultimate Name',\n",
        "                        'valor': name,\n",
        "                        'stage': 'Priority_Global',\n",
        "                        'palavra_match': palavra_match\n",
        "                    })\n",
        "                    linhas_processadas.add(idx)\n",
        "\n",
        "    print(\"🔍 ETAPA 0.2 - Busca prioritária COMPLETA em Domestic Ultimate Name...\")\n",
        "    if \"Domestic Ultimate Name\" in list_mapping.columns:\n",
        "        for idx, name in enumerate(list_mapping[\"Domestic Ultimate Name\"]):\n",
        "            if idx not in linhas_processadas:\n",
        "                resultado, palavra_match = busca_contextual_inteligente(name, priority_words)\n",
        "                if resultado:\n",
        "                    priority_results.append({\n",
        "                        'lm_idx': idx,\n",
        "                        'coluna': 'Domestic Ultimate Name',\n",
        "                        'valor': name,\n",
        "                        'stage': 'Priority_Domestic',\n",
        "                        'palavra_match': palavra_match\n",
        "                    })\n",
        "                    linhas_processadas.add(idx)\n",
        "\n",
        "    print(\"🔍 ETAPA 0.3 - Busca prioritária COMPLETA em Account Name...\")\n",
        "    if \"Account Name\" in list_mapping.columns:\n",
        "        for idx, name in enumerate(list_mapping[\"Account Name\"]):\n",
        "            if idx not in linhas_processadas:\n",
        "                resultado, palavra_match = busca_contextual_inteligente(name, priority_words)\n",
        "                if resultado:\n",
        "                    priority_results.append({\n",
        "                        'lm_idx': idx,\n",
        "                        'coluna': 'Account Name',\n",
        "                        'valor': name,\n",
        "                        'stage': 'Priority_Account',\n",
        "                        'palavra_match': palavra_match\n",
        "                    })\n",
        "                    linhas_processadas.add(idx)\n",
        "\n",
        "    return priority_results, linhas_processadas\n",
        "\n",
        "# ==== FUNÇÃO DE BUSCA PRIORITÁRIA NO ALL PRODUCTS ====\n",
        "def busca_prioritaria_all_products(all_products_df, priority_words):\n",
        "    \"\"\"\n",
        "    Busca palavras-chave prioritárias no All Products\n",
        "    \"\"\"\n",
        "    priority_results = []\n",
        "\n",
        "    if all_products_df.empty:\n",
        "        return priority_results\n",
        "\n",
        "    print(\"🔍 ETAPA 0.4 - Busca prioritária COMPLETA em All Products...\")\n",
        "\n",
        "    # Verificar coluna Account Name no All Products\n",
        "    account_col = None\n",
        "    for col in all_products_df.columns:\n",
        "        if 'account' in str(col).lower() and 'name' in str(col).lower():\n",
        "            account_col = col\n",
        "            break\n",
        "\n",
        "    if account_col:\n",
        "        for idx, name in enumerate(all_products_df[account_col]):\n",
        "            resultado, palavra_match = busca_contextual_inteligente(name, priority_words)\n",
        "            if resultado:\n",
        "                priority_results.append({\n",
        "                    'ap_idx': idx,\n",
        "                    'coluna': account_col,\n",
        "                    'valor': name,\n",
        "                    'stage': 'Priority_AllProducts',\n",
        "                    'palavra_match': palavra_match\n",
        "                })\n",
        "\n",
        "    return priority_results\n",
        "\n",
        "# ==== INTEGRAR ALL PRODUCTS PRIORITÁRIOS COM LIST MAPPING ====\n",
        "def integrar_ap_com_lm(priority_ap_df, list_mapping):\n",
        "    \"\"\"\n",
        "    Integra matches prioritários do All Products com List Mapping\n",
        "    \"\"\"\n",
        "    integrated_results = []\n",
        "\n",
        "    if priority_ap_df.empty or list_mapping.empty:\n",
        "        return integrated_results\n",
        "\n",
        "    # Criar mapeamento de Account Name para facilitar busca\n",
        "    lm_account_map = {}\n",
        "    for idx, row in list_mapping.iterrows():\n",
        "        account_name = str(row.get('Account Name', '')).strip()\n",
        "        if account_name:\n",
        "            lm_account_map[account_name] = idx\n",
        "\n",
        "    for _, ap_row in priority_ap_df.iterrows():\n",
        "        account_name_ap = str(ap_row.get('Account_Name_AP', '')).strip()\n",
        "\n",
        "        if account_name_ap in lm_account_map:\n",
        "            lm_idx = lm_account_map[account_name_ap]\n",
        "            lm_row = list_mapping.iloc[lm_idx]\n",
        "\n",
        "            # Adicionar todos os dados relevantes\n",
        "            integrated_results.append({\n",
        "                'lm_idx': lm_idx,\n",
        "                'ap_idx': ap_row['ap_idx'],\n",
        "                'Account_Name_LM': lm_row.get('Account Name', ''),\n",
        "                'Account_Name_AP': account_name_ap,\n",
        "                'Global_Ultimate_Name': lm_row.get('Global Ultimate Name', ''),\n",
        "                'Domestic_Ultimate_Name': lm_row.get('Domestic Ultimate Name', ''),\n",
        "                'Product_Code_AP': ap_row.get('Product_Code_AP', ''),\n",
        "                'Product_Description_AP': ap_row.get('Product_Description_AP', ''),\n",
        "                'Sales_AP': ap_row.get('Sales_AP', ''),\n",
        "                'stage': ap_row['stage'],\n",
        "                'palavra_match': ap_row['palavra_match']\n",
        "            })\n",
        "\n",
        "# ==== CARREGAR DADOS ====\n",
        "print(\"Carregando dados...\")\n",
        "\n",
        "file_business_group = \"/media/Arquivos_ULA/Business_Group.xlsx\"\n",
        "file_listmap_path = \"/media/Arquivos_ULA/List_Mapping.xlsb\"\n",
        "file_allprod_path = \"/media/Arquivos_ULA/All_Products.xlsb\"\n",
        "\n",
        "def read_excel_smart(path, sheet_name=None, usecols=None, header=0):\n",
        "    engine = \"pyxlsb\" if str(path).lower().endswith(\".xlsb\") else None\n",
        "    try:\n",
        "        if engine:\n",
        "            return pd.read_excel(path, sheet_name=sheet_name, usecols=usecols, header=header, engine=engine)\n",
        "        else:\n",
        "            return pd.read_excel(path, sheet_name=sheet_name, usecols=usecols, header=header)\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao carregar {sheet_name}: {e}\")\n",
        "        # Tentar descobrir os nomes das planilhas disponíveis\n",
        "        try:\n",
        "            if engine:\n",
        "                xl = pd.ExcelFile(path, engine=engine)\n",
        "            else:\n",
        "                xl = pd.ExcelFile(path)\n",
        "            print(f\"Planilhas disponíveis em {path}: {xl.sheet_names}\")\n",
        "            xl.close()\n",
        "        except Exception:\n",
        "            print(f\"Não foi possível ler as planilhas de {path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Carregar Business Group\n",
        "subs_df = read_excel_smart(file_business_group, sheet_name=\"Subs_Country\")\n",
        "if subs_df.empty:\n",
        "    # Tentar carregar sem especificar sheet_name\n",
        "    subs_df = read_excel_smart(file_business_group)\n",
        "    if not subs_df.empty:\n",
        "        print(\"Business Group carregado (planilha padrão)\")\n",
        "    else:\n",
        "        # Criar DataFrame vazio para continuar\n",
        "        subs_df = pd.DataFrame(columns=[\"Subsidiary List\", \"Country\", \"Region\"])\n",
        "        print(\"Business Group não pôde ser carregado, usando DataFrame vazio\")\n",
        "\n",
        "subs_df.columns = subs_df.columns.str.strip()\n",
        "country_list = subs_df[\"Country\"].astype(str).str.strip().dropna().unique().tolist() if not subs_df.empty else []\n",
        "\n",
        "if not subs_df.empty:\n",
        "    subs_df[\"Subs_clean\"] = subs_df[\"Subsidiary List\"].apply(lambda x: clean_name(x, country_list))\n",
        "    subs_lad = subs_df[subs_df[\"Region\"].astype(str).str.upper() == \"LAD\"].copy()\n",
        "    subs_all = subs_df.copy()\n",
        "else:\n",
        "    # Criar DataFrames vazios\n",
        "    subs_lad = pd.DataFrame(columns=[\"Subs_clean\", \"Subsidiary List\", \"Country\"])\n",
        "    subs_all = pd.DataFrame(columns=[\"Subs_clean\", \"Subsidiary List\", \"Country\"])\n",
        "\n",
        "# Preparar listas de subsidiárias\n",
        "subs_lad_clean_list = subs_lad[\"Subs_clean\"].tolist() if not subs_lad.empty else []\n",
        "subs_lad_orig_list = subs_lad[\"Subsidiary List\"].tolist() if not subs_lad.empty else []\n",
        "subs_lad_tokens = [tokenize(s) for s in subs_lad_clean_list] if subs_lad_clean_list else []\n",
        "\n",
        "subs_all_clean_list = subs_all[\"Subs_clean\"].tolist() if not subs_all.empty else []\n",
        "subs_all_orig_list = subs_all[\"Subsidiary List\"].tolist() if not subs_all.empty else []\n",
        "subs_all_tokens = [tokenize(s) for s in subs_all_clean_list] if subs_all_clean_list else []\n",
        "\n",
        "# Carregar List Mapping\n",
        "list_mapping = read_excel_smart(file_listmap_path, sheet_name=\"List Mapping\", header=1, usecols=\"A:J\")\n",
        "if list_mapping.empty:\n",
        "    # Tentar carregar sem header específico\n",
        "    list_mapping = read_excel_smart(file_listmap_path, usecols=\"A:J\")\n",
        "    if not list_mapping.empty:\n",
        "        print(\"List Mapping carregado (sem header específico)\")\n",
        "    else:\n",
        "        # Criar DataFrame vazio para continuar\n",
        "        list_mapping = pd.DataFrame(columns=[\"Global Ultimate Name\", \"Domestic Ultimate Name\", \"Account Name\"])\n",
        "        print(\"List Mapping não pôde ser carregado, usando DataFrame vazio\")\n",
        "\n",
        "cols_AJ = list_mapping.columns.tolist() if not list_mapping.empty else []\n",
        "\n",
        "# Limpar colunas do List Mapping\n",
        "if not list_mapping.empty:\n",
        "    list_mapping[\"Global_clean\"] = list_mapping[\"Global Ultimate Name\"].apply(lambda x: clean_name(x, country_list))\n",
        "    list_mapping[\"Domestic_clean\"] = list_mapping[\"Domestic Ultimate Name\"].apply(lambda x: clean_name(x, country_list))\n",
        "    list_mapping[\"Account_clean\"] = list_mapping[\"Account Name\"].apply(lambda x: clean_name(remove_parentheses(x), country_list))\n",
        "else:\n",
        "    # criar Series vazias com dtype str para evitar erros de atribuição\n",
        "    list_mapping[\"Global_clean\"] = pd.Series(dtype=str)\n",
        "    list_mapping[\"Domestic_clean\"] = pd.Series(dtype=str)\n",
        "    list_mapping[\"Account_clean\"] = pd.Series(dtype=str)\n",
        "\n",
        "# ==== CARREGAR ALL PRODUCTS COM NOME CORRETO DA PLANILHA ====\n",
        "print(\"Carregando All Products...\")\n",
        "all_products = read_excel_smart(file_allprod_path, sheet_name=\"LAD Oppty - Revenue Line Detail\", header=0)\n",
        "\n",
        "if all_products.empty:\n",
        "    print(\"Planilha 'LAD Oppty - Revenue Line Detail' não encontrada, procurando alternativas...\")\n",
        "    # Tentar encontrar a planilha correta\n",
        "    all_products_sheets = []\n",
        "    try:\n",
        "        xl = pd.ExcelFile(file_allprod_path, engine=\"pyxlsb\")\n",
        "        all_products_sheets = xl.sheet_names\n",
        "        xl.close()\n",
        "        print(f\"Planilhas disponíveis em All Products: {all_products_sheets}\")\n",
        "\n",
        "        # Tentar encontrar planilha com nome similar\n",
        "        for sheet in all_products_sheets:\n",
        "            if \"revenue\" in sheet.lower() or \"oppty\" in sheet.lower() or \"lad\" in sheet.lower():\n",
        "                print(f\" Tentando planilha: {sheet}\")\n",
        "                all_products = read_excel_smart(file_allprod_path, sheet_name=sheet, header=0)\n",
        "                if not all_products.empty:\n",
        "                    print(f\"All Products carregado da planilha: {sheet}\")\n",
        "                    break\n",
        "    except Exception as e:\n",
        "        print(f\"Não foi possível ler All Products: {e}\")\n",
        "        all_products = pd.DataFrame()\n",
        "\n",
        "if all_products.empty:\n",
        "    print(\"All Products não encontrado, continuando sem integração\")\n",
        "    account_name_map = {}\n",
        "else:\n",
        "    print(f\"All Products carregado: {len(all_products)} linhas\")\n",
        "    # Criar mapeamento de Account Name para All Products\n",
        "    account_name_map = {}\n",
        "    for idx, row in all_products.iterrows():\n",
        "        account_name = str(row.get('Account Name', '')).strip()\n",
        "        if account_name:\n",
        "            account_name_map[account_name] = row.to_dict()\n",
        "\n",
        "# ==== EXECUTAR BUSCA PRIORITÁRIA COMPLETA ====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INICIANDO BUSCA PRIORITÁRIA COMPLETA EM CAMADAS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not list_mapping.empty:\n",
        "    priority_results, linhas_prioritarias = busca_prioritaria_completa_camadas(list_mapping, PALAVRAS_CHAVE_PRIORITARIAS)\n",
        "    priority_df = pd.DataFrame(priority_results)\n",
        "    print(f\" {len(priority_df)} matches prioritários encontrados (TODOS!)\")\n",
        "\n",
        "    if not priority_df.empty:\n",
        "        print(\"\\nDistribuição por camada:\")\n",
        "        print(priority_df['stage'].value_counts())\n",
        "        display(priority_df.head(20))\n",
        "    else:\n",
        "        print(\"Nenhum match prioritário encontrado\")\n",
        "else:\n",
        "    print(\"List Mapping vazio, pulando busca prioritária\")\n",
        "    priority_df = pd.DataFrame()\n",
        "    linhas_prioritarias = set()\n",
        "\n",
        "# ==== EXECUTAR BUSCA PRIORITÁRIA NO ALL PRODUCTS ====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"INICIANDO BUSCA PRIORITÁRIA NO ALL PRODUCTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not all_products.empty:\n",
        "    priority_results_ap = busca_prioritaria_all_products(all_products, PALAVRAS_CHAVE_PRIORITARIAS)\n",
        "    priority_ap_df = pd.DataFrame(priority_results_ap)\n",
        "    print(f\" {len(priority_ap_df)} matches prioritários encontrados no All Products\")\n",
        "\n",
        "    if not priority_ap_df.empty:\n",
        "        print(\"\\n Matches prioritários no All Products:\")\n",
        "        display(priority_ap_df.head(20))\n",
        "\n",
        "        # Integrar com dados completos do All Products\n",
        "        priority_ap_complete = []\n",
        "        for _, row in priority_ap_df.iterrows():\n",
        "            ap_idx = row['ap_idx']\n",
        "            ap_row = all_products.iloc[ap_idx]\n",
        "\n",
        "            priority_ap_complete.append({\n",
        "                'ap_idx': ap_idx,\n",
        "                'Account_Name_AP': ap_row.get('Account Name', ''),\n",
        "                'Product_Code_AP': ap_row.get('Product Code', ''),\n",
        "                'Product_Description_AP': ap_row.get('Product Description', ''),\n",
        "                'Sales_AP': ap_row.get('Sales', ''),\n",
        "                'Quantity_AP': ap_row.get('Quantity', ''),\n",
        "                'Customer_Segment_AP': ap_row.get('Customer Segment', ''),\n",
        "                'stage': row['stage'],\n",
        "                'palavra_match': row['palavra_match']\n",
        "            })\n",
        "\n",
        "        priority_ap_final_df = pd.DataFrame(priority_ap_complete)\n",
        "        print(f\" {len(priority_ap_final_df)} registros completos do All Products\")\n",
        "\n",
        "    else:\n",
        "        print(\" Nenhum match prioritário encontrado no All Products\")\n",
        "        priority_ap_final_df = pd.DataFrame()\n",
        "else:\n",
        "    print(\" All Products vazio, pulando busca prioritária\")\n",
        "    priority_ap_final_df = pd.DataFrame()\n",
        "\n",
        "# ==== INTEGRAR ALL PRODUCTS PRIORITÁRIOS COM LIST MAPPING ====\n",
        "if not priority_ap_final_df.empty and not list_mapping.empty:\n",
        "    integrated_ap_lm = integrar_ap_com_lm(priority_ap_final_df, list_mapping)\n",
        "    integrated_ap_lm_df = pd.DataFrame(integrated_ap_lm)\n",
        "    print(f\"{len(integrated_ap_lm_df)} registros integrados (All Products + List Mapping)\")\n",
        "\n",
        "    if not integrated_ap_lm_df.empty:\n",
        "        display(integrated_ap_lm_df.head(10))\n",
        "else:\n",
        "    integrated_ap_lm_df = pd.DataFrame()\n",
        "\n",
        "# ==== ENCONTRAR SUBSIDIÁRIAS PARA MATCHES PRIORITÁRIOS ====\n",
        "print(\"\\nEncontrando subsidiárias para matches prioritários...\")\n",
        "\n",
        "if (not subs_all.empty) and (not priority_df.empty):\n",
        "    subs_pais_map = dict(zip(subs_all[\"Subsidiary List\"], subs_all[\"Country\"])) if not subs_all.empty else {}\n",
        "    priority_com_subs = []\n",
        "\n",
        "    for _, row in priority_df.iterrows():\n",
        "        lm_idx = row['lm_idx']\n",
        "        lm_row = list_mapping.iloc[lm_idx]\n",
        "\n",
        "        # Buscar TODAS as subsidiárias correspondentes\n",
        "        target_clean = lm_row.get(\"Global_clean\") or lm_row.get(\"Domestic_clean\") or lm_row.get(\"Account_clean\") or \"\"\n",
        "\n",
        "        matches_subs = []\n",
        "        for subs_idx, subs_clean in enumerate(subs_all_clean_list):\n",
        "            if not subs_clean:\n",
        "                continue\n",
        "            score = fuzz.token_set_ratio(target_clean, subs_clean)\n",
        "            if score >= MATCH_THRESHOLD:\n",
        "                subs_name = subs_all_orig_list[subs_idx]\n",
        "                matches_subs.append({\n",
        "                    'subsidiary': subs_name,\n",
        "                    'score': score,\n",
        "                    'pais': subs_pais_map.get(subs_name, 'Desconhecido')\n",
        "                })\n",
        "\n",
        "        # Ordenar por score e pegar as melhores\n",
        "        matches_subs.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "        for match in matches_subs[:5]:  # Top 5 matches por linha\n",
        "            # Criar um dicionário com TODAS as colunas de A:J\n",
        "            lm_data = {}\n",
        "            for col in cols_AJ:\n",
        "                lm_data[f'lm_{col}'] = lm_row.get(col, None)\n",
        "\n",
        "            priority_com_subs.append({\n",
        "                'lm_idx': lm_idx,\n",
        "                **lm_data,  # Incluir todas as colunas de A:J\n",
        "                'subsidiary': match['subsidiary'],\n",
        "                'subs_country': match['pais'],\n",
        "                'score': match['score'],\n",
        "                'match_type': 'PRIORITY_KEYWORD',\n",
        "                'stage': row['stage'],\n",
        "                'palavra_match': row['palavra_match']\n",
        "            })\n",
        "\n",
        "    priority_final_df = pd.DataFrame(priority_com_subs)\n",
        "    print(f\" {len(priority_final_df)} matches prioritários com subsidiárias encontrados\")\n",
        "\n",
        "    # Mostrar as colunas de A:J na exibição\n",
        "    if not priority_final_df.empty:\n",
        "        print(\"\\n Exibindo matches prioritários com todas as colunas A:J:\")\n",
        "        display_cols = ['lm_idx', 'stage', 'palavra_match'] + [f'lm_{col}' for col in cols_AJ] + ['subsidiary', 'subs_country', 'score']\n",
        "        display(priority_final_df[display_cols].head(20))\n",
        "else:\n",
        "    print(\" Dados insuficientes para buscar subsidiárias prioritárias\")\n",
        "    priority_final_df = pd.DataFrame()\n",
        "\n",
        "# ==== INTEGRAÇÃO COM ALL PRODUCTS ====\n",
        "print(\"\\nINTEGRANDO COM ALL PRODUCTS...\")\n",
        "\n",
        "if not priority_final_df.empty and account_name_map:\n",
        "    # Adicionar dados do All Products aos matches prioritários\n",
        "    priority_with_all_products = []\n",
        "    for _, row in priority_final_df.iterrows():\n",
        "        account_name = str(row.get('lm_Account Name', '')).strip()\n",
        "        all_products_data = account_name_map.get(account_name, {})\n",
        "\n",
        "        new_row = row.to_dict()\n",
        "        # Adicionar campos relevantes do All Products\n",
        "        for field in ['Product Code', 'Product Description', 'Sales', 'Quantity', 'Customer Segment']:\n",
        "            new_row[f'all_products_{field}'] = all_products_data.get(field, 'N/A')\n",
        "\n",
        "        priority_with_all_products.append(new_row)\n",
        "\n",
        "    priority_final_df = pd.DataFrame(priority_with_all_products)\n",
        "    print(f\"Integração com All Products concluída: {len(priority_final_df)} registros\")\n",
        "else:\n",
        "    print(\" Não há dados para integrar com All Products\")\n",
        "\n",
        "# ==== FUNÇÃO DE MATCHING PARA ETAPAS NORMAIS ====\n",
        "def match_LM_column_to_subs(list_series, subs_clean_list, subs_orig_list, subs_tokens_list, excluded_indices=None):\n",
        "    \"\"\"Função de matching com suporte para excluir índices já utilizados\"\"\"\n",
        "    if excluded_indices is None:\n",
        "        excluded_indices = set()\n",
        "\n",
        "    matches_map = {}\n",
        "    used_subs = set()\n",
        "\n",
        "    for lm_idx, target in list_series.items():\n",
        "        if lm_idx in excluded_indices:\n",
        "            continue\n",
        "        t = str(target or \"\").strip()\n",
        "        if not t:\n",
        "            continue\n",
        "\n",
        "        cand_idx = generate_candidate_idx(t, subs_clean_list, subs_tokens_list)\n",
        "\n",
        "        if not cand_idx:\n",
        "            # proteger caso subs_clean_list vazio\n",
        "            if not subs_clean_list:\n",
        "                continue\n",
        "            raw = process.extract(t, subs_clean_list, scorer=fuzz.token_set_ratio, limit=TOPN_PER_ROW)\n",
        "            cand_idx = []\n",
        "            for x in raw:\n",
        "                if len(x) >= 3 and isinstance(x[2], int):\n",
        "                    cand_idx.append(x[2])\n",
        "                else:\n",
        "                    try:\n",
        "                        cand_idx.append(subs_clean_list.index(x[0]))\n",
        "                    except Exception:\n",
        "                        continue\n",
        "\n",
        "        cand_idx = [ci for ci in dict.fromkeys(ci for ci in cand_idx if ci is not None)]\n",
        "        accepted = []\n",
        "        cand_strings = [subs_clean_list[i] for i in cand_idx] if cand_idx else []\n",
        "\n",
        "        if cand_strings:\n",
        "            raw2 = process.extract(t, cand_strings, scorer=fuzz.token_set_ratio, limit=min(TOPN_PER_ROW, len(cand_strings)))\n",
        "            for entry in raw2:\n",
        "                if len(entry) >= 3:\n",
        "                    choice_str, prelim, pos = entry[0], entry[1], entry[2]\n",
        "                else:\n",
        "                    choice_str, prelim = entry[0], entry[1]\n",
        "                    pos = cand_strings.index(choice_str)\n",
        "                real_idx = cand_idx[pos]\n",
        "                s_clean = subs_clean_list[real_idx]\n",
        "                ok, best, jw, cos = accept_pair(t, s_clean)\n",
        "                if ok:\n",
        "                    accepted.append((subs_orig_list[real_idx], int(round(best)), float(jw), (None if cos is None else float(cos))))\n",
        "\n",
        "        if accepted:\n",
        "            matches_map[lm_idx] = accepted\n",
        "            for s, _best, _jw, _cos in accepted:\n",
        "                used_subs.add(s)\n",
        "\n",
        "    return matches_map, used_subs\n",
        "\n",
        "# ==== ETAPAS NORMAIS (apenas se houver dados) ====\n",
        "if (not list_mapping.empty) and subs_lad_clean_list:\n",
        "    # ==== ETAPA 1 - GLOBAL (excluindo prioritários) ====\n",
        "    print(\"\\n ETAPA 1 - List Mapping: Global VS Subsidiary (LAD)\")\n",
        "    matches_stage1, used_subs_1 = match_LM_column_to_subs(\n",
        "        list_mapping[\"Global_clean\"],\n",
        "        subs_lad_clean_list,\n",
        "        subs_lad_orig_list,\n",
        "        subs_lad_tokens,\n",
        "        excluded_indices=linhas_prioritarias\n",
        "    )\n",
        "\n",
        "    if matches_stage1:\n",
        "        idxs1 = sorted(matches_stage1.keys())\n",
        "        stage1_df = list_mapping.loc[idxs1, cols_AJ].copy()\n",
        "        stage1_df.insert(0, \"RowIndex\", idxs1)\n",
        "        stage1_df[\"Matched_Subsidiaries\"] = [[m[0] for m in matches_stage1[i]] for i in idxs1]\n",
        "        stage1_df[\"Best_Scores\"] = [[m[1] for m in matches_stage1[i]] for i in idxs1]\n",
        "        stage1_df[\"JW\"] = [[m[2] for m in matches_stage1[i]] for i in idxs1]\n",
        "        stage1_df[\"Cosine\"] = [[m[3] for m in matches_stage1[i]] for i in idxs1]\n",
        "        stage1_df[\"Matched_Count\"] = stage1_df[\"Matched_Subsidiaries\"].apply(len)\n",
        "\n",
        "        # Integrar com All Products\n",
        "        if account_name_map:\n",
        "            stage1_with_products = []\n",
        "            for _, row in stage1_df.iterrows():\n",
        "                account_name = str(row.get('Account Name', '')).strip()\n",
        "                all_products_data = account_name_map.get(account_name, {})\n",
        "\n",
        "                new_row = row.to_dict()\n",
        "                for field in ['Product Code', 'Product Description', 'Sales', 'Quantity', 'Customer Segment']:\n",
        "                    new_row[f'all_products_{field}'] = all_products_data.get(field, 'N/A')\n",
        "                stage1_with_products.append(new_row)\n",
        "\n",
        "            stage1_df = pd.DataFrame(stage1_with_products)\n",
        "        print(f\"Etapa 1: {len(stage1_df)} linhas encontradas\")\n",
        "    else:\n",
        "        stage1_df = pd.DataFrame(columns=[\"RowIndex\"] + cols_AJ + [\"Matched_Subsidiaries\", \"Best_Scores\", \"JW\", \"Cosine\", \"Matched_Count\"])\n",
        "        print(\" Nenhuma linha encontrada na Etapa 1\")\n",
        "\n",
        "    matched_idx_1 = set(matches_stage1.keys() if matches_stage1 else [])\n",
        "    used_subs_all = set(used_subs_1 if matches_stage1 else set())\n",
        "\n",
        "    # ==== ETAPA 2 - DOMESTIC (excluindo prioritários + etapa 1) ====\n",
        "    print(\"\\nETAPA 2 - List Mapping: Domestic VS Subsidiary (ALL)\")\n",
        "    remaining_indices = matched_idx_1.union(linhas_prioritarias)\n",
        "    remaining_dom = list_mapping.drop(index=remaining_indices, errors=\"ignore\")\n",
        "\n",
        "    if not subs_all.empty:\n",
        "        subs2_mask = ~subs_all[\"Subsidiary List\"].isin(used_subs_all)\n",
        "        subs2_clean = subs_all.loc[subs2_mask, \"Subs_clean\"].tolist()\n",
        "        subs2_orig = subs_all.loc[subs2_mask, \"Subsidiary List\"].tolist()\n",
        "        subs2_tokens = [tokenize(s) for s in subs2_clean]\n",
        "    else:\n",
        "        subs2_clean, subs2_orig, subs2_tokens = [], [], []\n",
        "\n",
        "    matches_stage2, used_subs_2 = match_LM_column_to_subs(\n",
        "        remaining_dom[\"Domestic_clean\"],\n",
        "        subs2_clean,\n",
        "        subs2_orig,\n",
        "        subs2_tokens,\n",
        "        excluded_indices=linhas_prioritarias\n",
        "    )\n",
        "\n",
        "    if matches_stage2:\n",
        "        idxs2 = sorted(matches_stage2.keys())\n",
        "        stage2_df = list_mapping.loc[idxs2, cols_AJ].copy()\n",
        "        stage2_df.insert(0, \"RowIndex\", idxs2)\n",
        "        stage2_df[\"Matched_Subsidiaries\"] = [[m[0] for m in matches_stage2[i]] for i in idxs2]\n",
        "        stage2_df[\"Best_Scores\"] = [[m[1] for m in matches_stage2[i]] for i in idxs2]\n",
        "        stage2_df[\"JW\"] = [[m[2] for m in matches_stage2[i]] for i in idxs2]\n",
        "        stage2_df[\"Cosine\"] = [[m[3] for m in matches_stage2[i]] for i in idxs2]\n",
        "        stage2_df[\"Matched_Count\"] = stage2_df[\"Matched_Subsidiaries\"].apply(len)\n",
        "\n",
        "        # Integrar com All Products\n",
        "        if account_name_map:\n",
        "            stage2_with_products = []\n",
        "            for _, row in stage2_df.iterrows():\n",
        "                account_name = str(row.get('Account Name', '')).strip()\n",
        "                all_products_data = account_name_map.get(account_name, {})\n",
        "\n",
        "                new_row = row.to_dict()\n",
        "                for field in ['Product Code', 'Product Description', 'Sales', 'Quantity', 'Customer Segment']:\n",
        "                    new_row[f'all_products_{field}'] = all_products_data.get(field, 'N/A')\n",
        "                stage2_with_products.append(new_row)\n",
        "\n",
        "            stage2_df = pd.DataFrame(stage2_with_products)\n",
        "        print(f\"Etapa 2: {len(stage2_df)} linhas encontradas\")\n",
        "    else:\n",
        "        stage2_df = pd.DataFrame(columns=[\"RowIndex\"] + cols_AJ + [\"Matched_Subsidiaries\", \"Best_Scores\", \"JW\", \"Cosine\", \"Matched_Count\"])\n",
        "        print(\"Nenhuma linha encontrada na Etapa 2\")\n",
        "\n",
        "    matched_idx_2 = set(matches_stage2.keys() if matches_stage2 else [])\n",
        "    used_subs_all.update(used_subs_2 if matches_stage2 else set())\n",
        "\n",
        "# ==== ETAPA 3 - ACCOUNT (excluindo prioritários + etapas 1+2) ====\n",
        "print(\"\\n ETAPA 3 - List Mapping: Account VS Subsidiary (ALL)\")\n",
        "remaining_indices = matched_idx_1.union(matched_idx_2).union(linhas_prioritarias)\n",
        "remaining_acc = list_mapping.drop(index=remaining_indices, errors=\"ignore\")\n",
        "\n",
        "if not subs_all.empty:\n",
        "    subs3_mask = ~subs_all[\"Subsidiary List\"].isin(used_subs_all)\n",
        "    subs3_clean = subs_all.loc[subs3_mask, \"Subs_clean\"].tolist()\n",
        "    subs3_orig = subs_all.loc[subs3_mask, \"Subsidiary List\"].tolist()\n",
        "    subs3_tokens = [tokenize(s) for s in subs3_clean]\n",
        "else:\n",
        "    subs3_clean, subs3_orig, subs3_tokens = [], [], []\n",
        "\n",
        "matches_stage3, used_subs_3 = match_LM_column_to_subs(\n",
        "    remaining_acc[\"Account_clean\"],\n",
        "    subs3_clean,\n",
        "    subs3_orig,\n",
        "    subs3_tokens,\n",
        "    excluded_indices=linhas_prioritarias\n",
        ")\n",
        "\n",
        "if matches_stage3:\n",
        "    idxs3 = sorted(matches_stage3.keys())\n",
        "    stage3_df = list_mapping.loc[idxs3, cols_AJ].copy()\n",
        "    stage3_df.insert(0, \"RowIndex\", idxs3)\n",
        "    stage3_df[\"Matched_Subsidiaries\"] = [[m[0] for m in matches_stage3[i]] for i in idxs3]\n",
        "    stage3_df[\"Best_Scores\"] = [[m[1] for m in matches_stage3[i]] for i in idxs3]\n",
        "    stage3_df[\"JW\"] = [[m[2] for m in matches_stage3[i]] for i in idxs3]\n",
        "    stage3_df[\"Cosine\"] = [[m[3] for m in matches_stage3[i]] for i in idxs3]\n",
        "    stage3_df[\"Matched_Count\"] = stage3_df[\"Matched_Subsidiaries\"].apply(len)\n",
        "\n",
        "# Integrar com All Products\n",
        "    if account_name_map:\n",
        "        stage3_with_products = []\n",
        "        for _, row in stage3_df.iterrows():\n",
        "            account_name = str(row.get('Account Name', '')).strip()\n",
        "            all_products_data = account_name_map.get(account_name, {})\n",
        "\n",
        "            new_row = row.to_dict()\n",
        "            for field in ['Product Code', 'Product Description', 'Sales', 'Quantity', 'Customer Segment']:\n",
        "                new_row[f'all_products_{field}'] = all_products_data.get(field, 'N/A')\n",
        "            stage3_with_products.append(new_row)\n",
        "\n",
        "        stage3_df = pd.DataFrame(stage3_with_products)\n",
        "    print(f\"Etapa 3: {len(stage3_df)} linhas encontradas\")\n",
        "else:\n",
        "    stage3_df = pd.DataFrame(columns=[\"RowIndex\"] + cols_AJ + [\"Matched_Subsidiaries\", \"Best_Scores\", \"JW\", \"Cosine\", \"Matched_Count\"])\n",
        "    print(\"Nenhuma linha encontrada na Etapa 3\")\n",
        "\n",
        "# Verificação de dados insuficientes (se necessário)\n",
        "if not list_mapping.empty and not subs_lad_clean_list:\n",
        "    print(\"Dados insuficientes para etapas normais\")\n",
        "    stage1_df = pd.DataFrame()\n",
        "    stage2_df = pd.DataFrame()\n",
        "    stage3_df = pd.DataFrame()\n",
        "\n",
        "# ==== RESUMO FINAL ====\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"RESUMO FINAL - TODAS AS ETAPAS\")\n",
        "print(\"=\"*80)\n",
        "# Preparar resultados das etapas normais para incluir todas as colunas A:J\n",
        "def prepare_stage_df_with_all_columns(stage_df, stage_name):\n",
        "    if stage_df.empty:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Garantir que temos todas as colunas de A:J\n",
        "    result_df = stage_df.copy()\n",
        "    for col in cols_AJ:\n",
        "        if col not in result_df.columns:\n",
        "            # usar valores da list_mapping (quando RowIndex existe)\n",
        "            try:\n",
        "                result_df[col] = list_mapping[col].iloc[result_df['RowIndex']].values\n",
        "            except Exception:\n",
        "                result_df[col] = None\n",
        "\n",
        "    result_df['Stage'] = stage_name\n",
        "    return result_df\n",
        "\n",
        "stage1_df_full = prepare_stage_df_with_all_columns(stage1_df, \"Global\")\n",
        "stage2_df_full = prepare_stage_df_with_all_columns(stage2_df, \"Domestic\")\n",
        "stage3_df_full = prepare_stage_df_with_all_columns(stage3_df, \"Account\")\n",
        "\n",
        "# Adicionar colunas de top match\n",
        "for _df in [stage1_df_full, stage2_df_full, stage3_df_full]:\n",
        "    if not _df.empty:\n",
        "        _df[\"Top_Subsidiary\"] = _df[\"Matched_Subsidiaries\"].apply(lambda xs: xs[0] if isinstance(xs, list) and len(xs) > 0 else None)\n",
        "        _df[\"Top_Score\"] = _df[\"Best_Scores\"].apply(lambda xs: xs[0] if isinstance(xs, list) and len(xs) > 0 else None)\n",
        "\n",
        "# Combinar todos os resultados\n",
        "resumo_final = pd.concat([priority_final_df, stage1_df_full, stage2_df_full, stage3_df_full], ignore_index=True, sort=False)\n",
        "\n",
        "print(f\" Resumo completo: {len(resumo_final)} linhas totais\")\n",
        "print(f\"   - Priority: {len(priority_final_df)}\")\n",
        "print(f\"   - Global: {len(stage1_df_full)}\")\n",
        "print(f\"   - Domestic: {len(stage2_df_full)}\")\n",
        "print(f\"   - Account: {len(stage3_df_full)}\")\n",
        "\n",
        "if not resumo_final.empty:\n",
        "    # Mostrar colunas principais para visualização\n",
        "    display_cols = ['Stage', 'lm_Global Ultimate Name', 'lm_Domestic Ultimate Name', 'lm_Account Name',\n",
        "                   'subsidiary', 'subs_country', 'score', 'palavra_match', 'Top_Subsidiary', 'Top_Score']\n",
        "    display_cols = [col for col in display_cols if col in resumo_final.columns]\n",
        "    display(resumo_final[display_cols].head(20))\n",
        "else:\n",
        "    print(\"Nenhum resultado encontrado\")\n",
        "\n",
        "# =========================\n",
        "# ETAPA 4 - ALL PRODUCTS (cruzamento)\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ETAPA 4 - CRUZAMENTO COM ALL PRODUCTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def detect_col(df, regex_list):\n",
        "    if df is None or df.columns is None:\n",
        "        return None\n",
        "    cols = list(df.columns)\n",
        "    norm = {c: base_normalize(c) for c in cols}\n",
        "    for rgx in regex_list:\n",
        "        try:\n",
        "            pat = re.compile(rgx, flags=re.I)\n",
        "        except Exception:\n",
        "            continue\n",
        "        for c in cols:\n",
        "            try:\n",
        "                if pat.search(str(c)) or pat.search(norm[c]):\n",
        "                    return c\n",
        "            except Exception:\n",
        "                continue\n",
        "    return cols[0] if cols else None\n",
        "\n",
        "# Detectar colunas no All Products (regexes mais robustas)\n",
        "ap_customer_col = detect_col(all_products, [\n",
        "    r\"\\bcustomer\\s*name\\b\",\n",
        "    r\"\\bcust(?:omer)?\\s*name\\b\",\n",
        "    r\"\\bclient\\s*name\\b\",\n",
        "    r\"\\baccount\\s*name\\b\",\n",
        "])\n",
        "\n",
        "ap_registry_col = detect_col(all_products, [\n",
        "    r\"\\bcustomer\\b.*\\bregistry\\b.*\\bid\\b\",\n",
        "    r\"\\bregistry\\b.*\\bid\\b\",\n",
        "    r\"\\bcustomer\\b.*\\bid\\b\",\n",
        "    r\"\\bclient\\b.*\\bid\\b\",\n",
        "    r\"\\baccount\\b.*\\bid\\b\",\n",
        "])\n",
        "\n",
        "# Preparar dados do All Products\n",
        "if not all_products.empty:\n",
        "    ap = all_products.copy()\n",
        "    if ap_customer_col:\n",
        "        ap.rename(columns={ap_customer_col: \"AP_Customer_Name\"}, inplace=True)\n",
        "    else:\n",
        "        ap[\"AP_Customer_Name\"] = ap.index.astype(str)\n",
        "\n",
        "    if ap_registry_col:\n",
        "        ap.rename(columns={ap_registry_col: \"AP_Customer_Registry_ID\"}, inplace=True)\n",
        "\n",
        "    ap[\"AP_Customer_Name\"] = ap[\"AP_Customer_Name\"].astype(str)\n",
        "    ap[\"Customer_clean\"] = ap[\"AP_Customer_Name\"].apply(lambda x: clean_name(x, country_list))\n",
        "    ap_clean_list = ap[\"Customer_clean\"].tolist()\n",
        "    ap_tokens_list = [tokenize(s) for s in ap_clean_list]\n",
        "\n",
        "    print(f\"All Products preparado: {len(ap)} linhas\")\n",
        "else:\n",
        "    ap = pd.DataFrame()\n",
        "    ap_clean_list = []\n",
        "    ap_tokens_list = []\n",
        "    print(\"All Products vazio, pulando etapa 4\")\n",
        "\n",
        "# Função para encontrar matches por nome\n",
        "def name_matches_from_clean(target_clean):\n",
        "    ap_idx = []\n",
        "    if not target_clean or not ap_clean_list:\n",
        "        return ap_idx\n",
        "\n",
        "    cand_idx = generate_candidate_idx(target_clean, ap_clean_list, ap_tokens_list)\n",
        "\n",
        "    if not cand_idx:\n",
        "        raw = process.extract(target_clean, ap_clean_list, scorer=fuzz.token_set_ratio, limit=TOPN_PER_ROW)\n",
        "        cand_idx = []\n",
        "        for x in raw:\n",
        "            if len(x) >= 3 and isinstance(x[2], int):\n",
        "                cand_idx.append(x[2])\n",
        "            else:\n",
        "                try:\n",
        "                    cand_idx.append(ap_clean_list.index(x[0]))\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "    cand_idx = [ci for ci in dict.fromkeys(ci for ci in cand_idx if ci is not None)]\n",
        "    out = []\n",
        "\n",
        "    if cand_idx:\n",
        "        cand_strings = [ap_clean_list[i] for i in cand_idx]\n",
        "        raw2 = process.extract(target_clean, cand_strings, scorer=fuzz.token_set_ratio, limit=min(TOPN_PER_ROW, len(cand_strings)))\n",
        "        for entry in raw2:\n",
        "            if len(entry) >= 3:\n",
        "                _choice_str, _pre, pos = entry[0], entry[1], entry[2]\n",
        "            else:\n",
        "                _choice_str, _pre = entry[0], entry[1]\n",
        "                pos = cand_strings.index(_choice_str)\n",
        "            real_idx = cand_idx[pos]\n",
        "            s_clean = ap_clean_list[real_idx]\n",
        "            ok, best, jw, cos = accept_pair(target_clean, s_clean)\n",
        "            if ok:\n",
        "                out.append((real_idx, int(round(best)), float(jw), (None if cos is None else float(cos))))\n",
        "    return out\n",
        "\n",
        "# Função para encontrar matches por ID\n",
        "def exact_id_matches_from_values(id_value):\n",
        "    if id_value is None or (isinstance(id_value, float) and pd.isna(id_value)):\n",
        "        return []\n",
        "    if \"AP_Customer_Registry_ID\" not in ap.columns:\n",
        "        return []\n",
        "\n",
        "    key = str(id_value).strip()\n",
        "    return list(ap.index[ap[\"AP_Customer_Registry_ID\"].astype(str).str.strip() == key])\n",
        "\n",
        "# Construir matches para cada estágio\n",
        "def build_ap_matches_for_stage(stage_df, stage_label, lm_key_col_name):\n",
        "    rows = []\n",
        "    if stage_df is None or stage_df.empty:\n",
        "        return pd.DataFrame(rows)\n",
        "\n",
        "    for _, row in stage_df.iterrows():\n",
        "        lm_idx = row.get(\"RowIndex\")\n",
        "        if lm_idx is None:\n",
        "            continue\n",
        "        target_clean = list_mapping.loc[lm_idx, lm_key_col_name]\n",
        "\n",
        "        # Buscar matches por nome\n",
        "        by_name = name_matches_from_clean(target_clean)\n",
        "\n",
        "        # Buscar matches por ID\n",
        "        lm_ids = []\n",
        "        for c in list_mapping.columns:\n",
        "            if \"id\" in str(c).lower():\n",
        "                lm_ids.append(list_mapping.loc[lm_idx, c])\n",
        "\n",
        "        by_id = set()\n",
        "        for v in lm_ids:\n",
        "            for ap_idx in exact_id_matches_from_values(v):\n",
        "                by_id.add(ap_idx)\n",
        "\n",
        "        # Combinar resultados\n",
        "        name_idx_set = {i for (i, _best, _jw, _cos) in by_name}\n",
        "        both = name_idx_set.intersection(by_id)\n",
        "        only_name = name_idx_set - both\n",
        "        only_id = by_id - both\n",
        "\n",
        "        lm_payload = list_mapping.loc[lm_idx].to_dict()\n",
        "        lm_payload_pref = {f\"LM_{k}\": v for k, v in lm_payload.items()}\n",
        "        lm_payload_pref[\"Stage\"] = stage_label\n",
        "\n",
        "        # Adicionar matches\n",
        "        for i in both:\n",
        "            ap_payload = ap.loc[i].to_dict()\n",
        "            ap_payload_pref = {f\"AP_{k}\" if not str(k).startswith(\"AP_\") else k: v for k, v in ap_payload.items()}\n",
        "            _best = _jw = _cos = None\n",
        "            for j, b, jw, co in by_name:\n",
        "                if j == i:\n",
        "                    _best, _jw, _cos = b, jw, co\n",
        "                    break\n",
        "            rows.append({**lm_payload_pref, **ap_payload_pref, \"Match_Mode\": \"Both\", \"Name_Score\": _best, \"Name_JW\": _jw, \"Name_Cosine\": _cos})\n",
        "\n",
        "        for i, b, jw, co in by_name:\n",
        "            if i in both:\n",
        "                continue\n",
        "            ap_payload = ap.loc[i].to_dict()\n",
        "            ap_payload_pref = {f\"AP_{k}\" if not str(k).startswith(\"AP_\") else k: v for k, v in ap_payload.items()}\n",
        "            rows.append({**lm_payload_pref, **ap_payload_pref, \"Match_Mode\": \"Name\", \"Name_Score\": b, \"Name_JW\": jw, \"Name_Cosine\": co})\n",
        "\n",
        "        for i in only_id:\n",
        "            ap_payload = ap.loc[i].to_dict()\n",
        "            ap_payload_pref = {f\"AP_{k}\" if not str(k).startswith(\"AP_\") else k: v for k, v in ap_payload.items()}\n",
        "            rows.append({**lm_payload_pref, **ap_payload_pref, \"Match_Mode\": \"ID\", \"Name_Score\": None, \"Name_JW\": None, \"Name_Cosine\": None})\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "# Construir matches para cada estágio\n",
        "AP_Global_All = build_ap_matches_for_stage(stage1_df, \"Global\", \"Global_clean\")\n",
        "AP_Domestic_All = build_ap_matches_for_stage(stage2_df, \"Domestic\", \"Domestic_clean\")\n",
        "AP_Account_All = build_ap_matches_for_stage(stage3_df, \"Account\", \"Account_clean\")\n",
        "\n",
        "print(\"\\nEtapa 4 - Matches com ALL PRODUCTS:\")\n",
        "print(\" AP_Global_All  :\", len(AP_Global_All))\n",
        "print(\" AP_Domestic_All:\", len(AP_Domestic_All))\n",
        "print(\" AP_Account_All :\", len(AP_Account_All))\n",
        "\n",
        "# =========================\n",
        "# ETAPA 5 - Validação de Produtos\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\" ETAPA 5 - VALIDAÇÃO DE PRODUTOS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Carregar produtos do Summary\n",
        "try:\n",
        "    bayer_summary = read_excel_smart(file_business_group, sheet_name=\"Summary\", header=None, usecols=[1])\n",
        "    bayer_summary.columns = [\"Summary_ColB\"]\n",
        "    bayer_products_raw = bayer_summary[\"Summary_ColB\"].dropna().astype(str)\n",
        "\n",
        "    def clean_product(p):\n",
        "        s = base_normalize(p)\n",
        "        return re.sub(r\"\\s+\", \" \", s).strip()\n",
        "\n",
        "    bayer_products = bayer_products_raw.apply(clean_product)\n",
        "    bayer_products = bayer_products[bayer_products.str.len() > 0].unique().tolist()\n",
        "    print(f\"ℹ Produtos do Summary carregados: {len(bayer_products)} itens.\")\n",
        "except Exception as e:\n",
        "    bayer_products = []\n",
        "    print(\" Não foi possível ler produtos do Summary (coluna B).\", e)\n",
        "\n",
        "# Identificar colunas de produto no All Products\n",
        "def find_product_columns(ap_cols):\n",
        "    pats = r\"(product|produto|sku|item|material|brand|family|line|portfolio|description|descri[cç][aã]o|desc)\"\n",
        "    return [c for c in ap_cols if re.search(pats, str(c), flags=re.I)]\n",
        "\n",
        "product_cols_in_ap = find_product_columns(ap.columns.tolist()) if not ap.empty else []\n",
        "\n",
        "if not product_cols_in_ap:\n",
        "    guesses = [c for c in ap.columns if re.search(r\"(prod|sku|item|material|desc|brand)\", str(c), re.I)] if not ap.empty else []\n",
        "    product_cols_in_ap = guesses\n",
        "\n",
        "print(f\"ℹColunas de produto identificadas: {product_cols_in_ap}\")\n",
        "\n",
        "# Função para calcular score de produto\n",
        "def product_best_score(a: str, b: str) -> int:\n",
        "    a = str(a or \"\"); b = str(b or \"\")\n",
        "    a = re.sub(r\"\\s+\", \" \", base_normalize(a)).strip()\n",
        "    b = re.sub(r\"\\s+\", \" \", base_normalize(b)).strip()\n",
        "    if not a or not b:\n",
        "        return 0\n",
        "    s1 = fuzz.token_set_ratio(a, b)\n",
        "    s2 = fuzz.token_sort_ratio(a, b)\n",
        "    s3 = fuzz.partial_ratio(a, b)\n",
        "    return int(max(s1, s2, s3))\n",
        "\n",
        "# Parâmetros para validação de produtos\n",
        "PRODUCT_MIN = 50\n",
        "PRODUCT_MAX = 60\n",
        "\n",
        "# Validar produtos\n",
        "prod_rows = []\n",
        "AP_all_matches = pd.concat([AP_Global_All, AP_Domestic_All, AP_Account_All], ignore_index=True) if (((not AP_Global_All.empty) or (not AP_Domestic_All.empty) or (not AP_Account_All.empty))) else pd.DataFrame()\n",
        "\n",
        "if (not AP_all_matches.empty) and bayer_products and product_cols_in_ap:\n",
        "    for idx, r in AP_all_matches.iterrows():\n",
        "        ap_cust_name = r.get(\"AP_AP_Customer_Name\", r.get(\"AP_Customer_Name\", None))\n",
        "        ap_cust_id = r.get(\"AP_AP_Customer_Registry_ID\", r.get(\"AP_Customer_Registry_ID\", None))\n",
        "        stage_src = r.get(\"Stage\", None)\n",
        "\n",
        "        for pc in product_cols_in_ap:\n",
        "            ap_val = r.get(f\"AP_{pc}\", None)\n",
        "            if ap_val is None:\n",
        "                continue\n",
        "            if pd.isna(ap_val) or str(ap_val).strip() == \"\":\n",
        "                continue\n",
        "\n",
        "            for bp in bayer_products:\n",
        "                sc = product_best_score(ap_val, bp)\n",
        "                if PRODUCT_MIN <= sc <= PRODUCT_MAX:\n",
        "                    prod_rows.append({\n",
        "                        \"Stage\": stage_src,\n",
        "                        \"AP_Customer_Name\": ap_cust_name,\n",
        "                        \"AP_Customer_Registry_ID\": ap_cust_id,\n",
        "                        \"AP_Product_Column\": pc,\n",
        "                        \"AP_Product_Value\": ap_val,\n",
        "                        \"Summary_Product\": bp,\n",
        "                        \"Product_Score\": sc\n",
        "                    })\n",
        "\n",
        "AP_Products_Validation = pd.DataFrame(prod_rows)\n",
        "print(f\"Etapa 5 - Validação de Produtos (50-60): {len(AP_Products_Validation)} pares produto encontrados.\")\n",
        "\n",
        "# ==== EXPORTAÇÃO FINAL ====\n",
        "out_xlsx = \"/media/Arquivos_ULA/RESULTADO_FINAL_COMPLETO_PEPSICO.xlsx\"\n",
        "try:\n",
        "    with pd.ExcelWriter(out_xlsx, engine=\"xlsxwriter\") as writer:\n",
        "        if not priority_final_df.empty:\n",
        "            priority_final_df.to_excel(writer, sheet_name=\"Stage0_Priority\", index=False)\n",
        "        if not stage1_df.empty:\n",
        "            stage1_df.to_excel(writer, sheet_name=\"Stage1_Global\", index=False)\n",
        "        if not stage2_df.empty:\n",
        "            stage2_df.to_excel(writer, sheet_name=\"Stage2_Domestic\", index=False)\n",
        "        if not stage3_df.empty:\n",
        "            stage3_df.to_excel(writer, sheet_name=\"Stage3_Account\", index=False)\n",
        "        if not resumo_final.empty:\n",
        "            resumo_final.to_excel(writer, sheet_name=\"Resumo_Completo\", index=False)\n",
        "        if not priority_ap_final_df.empty:\n",
        "            priority_ap_final_df.to_excel(writer, sheet_name=\"AP_Priority_Matches\", index=False)\n",
        "        if not integrated_ap_lm_df.empty:\n",
        "            integrated_ap_lm_df.to_excel(writer, sheet_name=\"AP_LM_Integrated\", index=False)\n",
        "        if not AP_Global_All.empty:\n",
        "            AP_Global_All.to_excel(writer, sheet_name=\"AP_Global_All\", index=False)\n",
        "        if not AP_Domestic_All.empty:\n",
        "            AP_Domestic_All.to_excel(writer, sheet_name=\"AP_Domestic_All\", index=False)\n",
        "        if not AP_Account_All.empty:\n",
        "            AP_Account_All.to_excel(writer, sheet_name=\"AP_Account_All\", index=False)\n",
        "        if not AP_Products_Validation.empty:\n",
        "            AP_Products_Validation.to_excel(writer, sheet_name=\"AP_Products_Validation\", index=False)\n",
        "\n",
        "    print(f\"\\nArquivo exportado: {out_xlsx}\")\n",
        "    print(\"Processamento concluído com sucesso!\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro ao exportar arquivo: {e}\")\n",
        "    print(\"Processamento concluído com erros\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxB3iTKgLCja"
      },
      "source": [
        "# Nova seção"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}